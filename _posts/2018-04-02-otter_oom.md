---
layout: post
title: " 记一次Otter的OOM排查过程"
author: "Lu Yee"
date: 2018-04-01 16:03:32
categories: ["java"]
tags: ["java"]
description: >
  记一次Otter的OOM排查过程
---

最开始报错，出现“miss data with keys”,这个在FAQ里边有提到，channel有点多了。

为此新增加了一台node； 然后线上某个channel突然出现OOM,于是试着修改JVM参数，增大内存

内存从默认的3072m升级到8192m ,还是会出现oom,还好产生了dump文件，关键这个文件有点大6.8g，

文件太大了不好下载下来，线上的机器要通过跳板机，专线也是限速的，尝试过好几次都没成功，压缩完881M,传到200-300m网络就断了。

体验差，只能用jhat在本地分析。jhat效果还是不是太好，最后放弃了


```
# ls -lh  java_pid119697.hprof
-rw------- 1 data data 6.8G 4月   2 04:37 java_pid119697.hprof
```

想到了tar不是可以分包压缩解压缩么。

```
tar czf - java_pid119697.hprof  | split -b 100m  - test.tar.gz
# ls test.tar.gza* -lh
-rw-r--r-- 1 root root 100M 4月   2 11:31 test.tar.gzaa
-rw-r--r-- 1 root root 100M 4月   2 11:32 test.tar.gzab
-rw-r--r-- 1 root root 100M 4月   2 11:32 test.tar.gzac
-rw-r--r-- 1 root root 100M 4月   2 11:32 test.tar.gzad
-rw-r--r-- 1 root root 100M 4月   2 11:33 test.tar.gzae
-rw-r--r-- 1 root root 100M 4月   2 11:33 test.tar.gzaf
-rw-r--r-- 1 root root 100M 4月   2 11:33 test.tar.gzag
-rw-r--r-- 1 root root 100M 4月   2 11:33 test.tar.gzah
-rw-r--r-- 1 root root  81M 4月   2 11:34 test.tar.gzai
```

每个大小100m,总共分9个文件，都慢慢传下来，然后解压缩得到整个文件java_pid119697.hprof

```
cat test.tar.gza* | tar -jx
```

有dump文件还是挺方便的，直接用MAT打开
![](/images/otter_oom.png)
 
发现MemoryEventStoreWithBuffer里边Event实例有13332个，总大小达6.8G多.

准没错就是它了，结合canal以及otter的源码，修改：   
1. canal的参数(内存存储buffer记录数)，这个默认是32768；   
2. 还有pipeline的参数主批次大小，就是canal客户端的哪个batchSize
 
